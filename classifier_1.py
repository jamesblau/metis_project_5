# -*- coding: utf-8 -*-
"""classifier

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fCjuRt4WXWtP5W18adPDWWW5NSDo8YfT
"""

import re
import pickle
import numpy as np
from sklearn.model_selection import train_test_split
from keras.applications.vgg16 import VGG16
from keras.models import Sequential
from keras.layers import Flatten, Dense
from keras.preprocessing import image
from keras.utils import np_utils

from google.colab import drive
drive.mount('/content/gdrive')

project_dir = "gdrive/My Drive/metis_project_5"

!cd gdrive/My\ Drive/metis_project_5

!mkdir -p gdrive/My\ Drive/metis_project_5/labels/classifier

!mv gdrive/My\ Drive/easy_lar* gdrive/My\ Drive/metis_project_5/labels/classifier/

# Get easy labels

# with open('labels/classifier/easy_labels.csv') as f:
#     raw_easy_labels = f.read().split('\n')[1:-1]

# easy_label_regex = re.compile('^([^\d]*)([^.]*)\.png,(.*)$')

# easy_label_to_int = {'a': 0, 'g': 1, 'db': 2, 'ub': 3, 's': 4}

# easy_paths, easy_y = [], []
# for label in raw_easy_labels:
#     (category, frame, label) = label_regex.match(label).groups()
#     int_label = label_to_int.get(label)
#     if int_label and category == 'new_fox_':
#         path = f"data/larger_cropped/{frame}.png"
#         easy_paths.append(path)
#         easy_y.append(int_label)

# easy_num_categories = 5

# Get new easy_larger_cropped labels

# with open('labels/classifier/easy_larger_cropped_labels.csv') as f:
#     raw_elc_labels = f.read().split('\n')[1:-1]

with open(f"{project_dir}/labels/classifier/easy_larger_cropped_labels.csv") as f:
    raw_elc_labels = f.read().split('\n')[1:-1]

elc_label_regex = re.compile('^([^.]*)\.png,(.*)$')

elc_label_to_int = {
    'g': 0,     # count: 200
    'a': 1,     # count: 128
    's': 0,     # count: 7
    'aub': 1,   # count: 5
    'gdb': 0,   # count: 5
    'adb': 1,   # count: 2
    # 'l': 1,   # count: 1
}

elc_paths, elc_y = [], []
for label in raw_elc_labels:
    frame, label = elc_label_regex.match(label).groups()
    if label in elc_label_to_int.keys():
        int_label = elc_label_to_int[label]
        path = f"{project_dir}/data/larger_cropped/{frame}.png"
        elc_paths.append(path)
        elc_y.append(int_label)

elc_num_categories = 2

# Get hard labels

# with open('labels/classifier/labels.csv') as f:
#     raw_hard_labels = f.read().split('\n')[1:-1]

# hard_label_regex = re.compile('^([^\d]*)([^.]*)\.png,.,(.*)$')

# hard_label_to_int = {
#     'a': 0,
#     'adb': 1,
#     'anb': 2,
#     'aub': 3,
#     'ba': 4,
#     'd': 5,
#     'da': 6,
#     'dt': 7,
#     'g': 8,
#     'gdb': 9,
#     'h': 10,
#     'ha': 11,
#     'hg': 12,
#     'j': 13,
#     'l': 14,
#     'na': 15,
#     'p': 16,
#     's': 17,
#     'sd': 18,
#     't': 19,
#     'ua': 20,
#     'ut': 21,
# }

# hard_paths, hard_y = [], []
# for label in raw_hard_labels:
#     (category, frame, label) = hard_label_regex.match(label).groups()
#     int_label = label_to_int.get(label)
#     if int_label and category == 'new_fox_':
#         labels.add(label)
#         path = f"data/larger_cropped/{frame}.png"
#         paths.append(path)
#         hard_y.append(label)

# hard_num_categories = 22

# Choose labels

paths = elc_paths
y = elc_y
num_categories = elc_num_categories

# image augmentation
train_datagen = image.ImageDataGenerator(
    # rescale=1./255,
    featurewise_std_normalization=True,
    vertical_flip=True,
    validation_split=0.25)

# only applying scaling to the validation set
# val_datagen = ImageDataGenerator(rescale=1./255)

# training and validation gerators
train_gen = train_datagen.flow(X_train, y_train_cat)
val_gen = val_datagen.flow(X_val, y_val_cat)
epochs=100
# callbacks
es = EarlyStopping(monitor='val_loss', mode='min', patience=30, verbose=1, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', verbose=1, factor=0.2,
                              patience=10, cooldown=5, min_lr=0.0002)
NN.summary()
NN.fit_generator(train_gen, steps_per_epoch=100, epochs=epochs, callbacks=[es, reduce_lr], 
                 verbose=1, validation_data=val_gen, validation_steps=20)

# Prepare test/train data

img_hw = (350, 350)
img_dimensions = (350, 350, 3)

def prepare_image(img_path):
    img = image.load_img(img_path, target_size=img_hw)
    x = image.img_to_array(img)
    x = np.expand_dims(x, axis=0)
    # x = mobilenet_v2.preprocess_input(x)
    return x

X = np.array([prepare_image(path)[0] for path in paths])

X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=42)

y_tr_cat = np_utils.to_categorical(y_tr)

pickles_dir = project_dir + "/pickles"

!ls gdrive/My\ Drive/metis_project_5/pickles

pickles_dir = project_dir + "/pickles"

# with open(f'{pickles_dir}/X_tr.pickle', 'wb') as f:
#     pickle.dump(X_tr, f)

# with open(f'{pickles_dir}/X_te.pickle', 'wb') as f:
#     pickle.dump(X_te, f)

# with open(f'{pickles_dir}/y_tr.pickle', 'wb') as f:
#     pickle.dump(y_tr, f)

# with open(f'{pickles_dir}/y_tr_cat.pickle', 'wb') as f:
#     pickle.dump(y_tr_cat, f)

# with open(f'{pickles_dir}/y_te.pickle', 'wb') as f:
#     pickle.dump(y_te, f)

with open(f'{pickles_dir}/X_tr.pickle', 'rb') as f:
    X_tr = pickle.load(f)

with open(f'{pickles_dir}/X_te.pickle', 'rb') as f:
    X_te = pickle.load(f)

with open(f'{pickles_dir}/y_tr.pickle', 'rb') as f:
    y_tr = pickle.load(f)

with open(f'{pickles_dir}/y_tr_cat.pickle', 'rb') as f:
    y_tr_cat = pickle.load(f)

with open(f'{pickles_dir}/y_te.pickle', 'rb') as f:
    y_te = pickle.load(f)

# Get pretrained model

pretrained = VGG16(
    weights='imagenet',
    include_top=False,
    input_shape=img_dimensions,
)

# Build model

for layer in pretrained.layers[:17]:
    layer.trainable = False

model = Sequential()
model.add(pretrained)
model.add(Flatten())

if num_categories > 2:
    model.add(Dense(num_categories))
    loss='categorical_crossentropy',
else:
    model.add(Dense(1, activation='sigmoid'))
    loss='binary_crossentropy',

model.compile(
    loss=loss,
    optimizer='adam',
    metrics=['accuracy'],
)

# Train model

for _ in range(10):
  if num_categories > 2:
      model.fit(X_tr, y_tr_cat)
  else:
      model.fit(X_tr, y_tr)
      y_pr = model.predict_classes(X_te)[:,0]
      print(f"Test accuracy: {len(y_pr[y_pr == y_te]) / len(y_pr)}")

# with open('pickles/model_1.pickle', 'wb') as f:
    # pickle.dump(model, f)

with open('pickles/model_1.pickle', 'rb') as f:
    model = pickle.load(f)

print(f"Train zeros: {len(list(filter(lambda x: x == 0, y_tr)))}")
print(f"Train ones: {len(list(filter(lambda x: x == 1, y_tr)))}")
print(f"Test zeros: {len(list(filter(lambda x: x == 0, y_te)))}")
print(f"Test ones: {len(list(filter(lambda x: x == 1, y_te)))}")

for x in zip(y_pr, y_te):
  print(x)

# Predict and score

y_pr = model.predict_classes(X_te)[:,0]

print(f"Test accuracy: {len(y_pr[y_pr == y_te]) / len(y_pr)}")

assert(len(y_pr) == len(y_te))

y_tr_pr = model.predict_classes(X_tr)[:,0]

print(f"Train accuracy: {len(y_tr_pr[y_tr_pr == y_tr]) / len(y_tr_pr)}")

assert(len(y_tr_pr) == len(y_tr))

print(y_pr)
print(y_te)
print(y_tr_pr)
print(y_tr)

model.save_weights?